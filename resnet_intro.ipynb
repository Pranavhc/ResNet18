{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Network\n",
    "\n",
    "An Example: Resnet for Images.\n",
    "\n",
    "<div style=\"display:flex\">\n",
    "    <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/0*sGlmENAXIZhSqyFZ\"/>\n",
    "    <pre>\n",
    "        When using the Residual Network, our network looks like a series of blocks instead \n",
    "        of a series of layers. Each block has a series of layers, and the input of block gets\n",
    "        added to the output of the last layer in the block. This is called a skip connection.\n",
    "        The skip connection allows the network to learn the residual of the block, which is \n",
    "        the difference between the input and the output of the block. This makes it easier for\n",
    "        the network to learn the identity function, which is the function that maps the input to\n",
    "        the output. The skip connection also helps with the vanishing gradient problem, which is \n",
    "        when the gradient of the loss function becomes very small as it is propagated back through\n",
    "        the network. The skip connection allows the gradient to bypass the block and go directly\n",
    "        to the next block, which helps with the vanishing gradient problem. This allows the network\n",
    "        to be very deep network that can learn very complex functions, and it has been very successful\n",
    "        the tasks that require very deep networks.\n",
    "    </pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: ResNets don't necessarily have to be used for images. They can be used for any type of data, such as text or audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input   - torch.Size([1, 3, 180, 180])\n",
      "blocks  - torch.Size([1, 512, 6, 6])\n",
      "avgpool - torch.Size([1, 512, 1, 1])\n",
      "flatten - torch.Size([1, 512])\n",
      "linear  - torch.Size([1, 1])\n",
      "\n",
      "Pred: tensor([[-0.7718]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_chnl:int, out_chnl:int, downsample:nn.Module=None, dropout:float=0.5):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_chnl, out_chnl, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_chnl)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_chnl, out_chnl, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_chnl)\n",
    "        \n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.downsample = downsample\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            out = self.maxpool(out)              # downsamples by maxpooling by 2\n",
    "            residual = self.downsample(residual) # halves the size by strides of 2\n",
    "\n",
    "        out = out + residual\n",
    "        out = self.dropout(out)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "    \n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, input_chnl:int):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        in_chnls = [input_chnl, 32, 64, 128, 256]\n",
    "        out_chnls = [32, 64, 128, 256, 512]\n",
    "\n",
    "        downsamplers = [nn.Conv2d(i, o, kernel_size=1, stride=2, bias=False) for i,o in zip (in_chnls, out_chnls)]\n",
    "        blocks = [ResidualBlock(i, o, d) for i, o, d in zip(in_chnls, out_chnls, downsamplers)]\n",
    "\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"input   - {x.shape}\")\n",
    "        \n",
    "        x = self.blocks(x);   print(f\"blocks  - {x.shape}\")\n",
    "        x = self.avgpool(x);  print(f\"avgpool - {x.shape}\")\n",
    "        x = self.flatten(x);  print(f\"flatten - {x.shape}\")\n",
    "        x = self.linear(x);   print(f\"linear  - {x.shape}\\n\")\n",
    "        return x\n",
    "    \n",
    "model = ResNet(input_chnl=3)\n",
    "\n",
    "# (batch, channel, height, width)\n",
    "x = torch.randn(1, 3, 180, 180); y = torch.tensor([1.])\n",
    "\n",
    "with torch.no_grad(): print(f\"Pred: {model(x)}\") # test forward pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
